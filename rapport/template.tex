%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 2.0 (February 7, 2023)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% NOTE: The bibliography needs to be compiled using the biber engine.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------


\documentclass[
	a4paper, % Paper size, use either a4paper or letterpaper
	10pt, % Default font size, can also use 11pt or 12pt, although this is not recommended
	unnumberedsections, % Comment to enable section numbering
	twoside, % Two side traditional mode where headers and footers change between odd and even pages, comment this option to make them fixed
]{LTJournalArticle}


\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{makecell}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{stmaryrd}


\addbibresource{sample.bib} % BibLaTeX bibliography file

\runninghead{Shortened Running Article Title} % A shortened article title to appear in the running head, leave this command empty for no running head

\footertext{\textit{Journal of Biological Sampling} (2024) 12:533-684} % Text to appear in the footer, leave this command empty for no footer text

\setcounter{page}{1} % The page number of the first page, set this to a higher number if the article is to be part of an issue or larger work

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{An Article Title That Spans Multiple\\ Lines to Show Line Wrapping} % Article title, use manual lines breaks (\\) to beautify the layout

% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
\author{%
	John Smith\textsuperscript{1,2}, Robert Smith\textsuperscript{3} and Jane Smith\textsuperscript{1}\thanks{Corresponding author: \href{mailto:jane@smith.com}{jane@smith.com}\\ \textbf{Received:} October 20, 2023, \textbf{Published:} December 14, 2023}
}

% Affiliations are output in the \date{} command
\date{\footnotesize\textsuperscript{\textbf{1}}School of Chemistry, The University of Michigan\\ \textsuperscript{\textbf{2}}Physics Department, The University of Wisconsin\\ \textsuperscript{\textbf{3}}Biological Sciences Department, The University of Minnesota}

% Full-width abstract
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent porttitor arcu luctus, imperdiet urna iaculis, mattis eros. Pellentesque iaculis odio vel nisl ullamcorper, nec faucibus ipsum molestie. Sed dictum nisl non aliquet porttitor. Etiam vulputate arcu dignissim, finibus sem et, viverra nisl. Aenean luctus congue massa, ut laoreet metus ornare in. Nunc fermentum nisi imperdiet lectus tincidunt vestibulum at ac elit. Nulla mattis nisl eu malesuada suscipit. Aliquam arcu turpis, ultrices sed luctus ac, vehicula id metus. Morbi eu feugiat velit, et tempus augue. Proin ac mattis tortor. Donec tincidunt, ante rhoncus luctus semper, arcu lorem lobortis justo, nec convallis ante quam quis lectus. Aenean tincidunt sodales massa, et hendrerit tellus mattis ac. Sed non pretium nibh. Donec cursus maximus luctus. Vivamus lobortis eros et massa porta porttitor.
	\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the title section

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

\subsection{Contexte}

Les algorithmes de \textit{computer vision} pour la détection de points d'intérêt sont aujourd'hui extrêmement efficaces, en particulier depuis la publication de SIFT [\ref{cite:sift}] en 2004, avec des applications dans de nombreux domaines : médecine, logistique et imagerie numérique pour ne citer qu'eux.
Néanmoins, les algorithmes existants semblent inadaptatés lorsqu'il s'agit de traiter des images sous-marines, et ce principalement à cause de la perte de contraste et du manque de luminosité aux profondeurs considérées.
Un domaine dans lequel cette lacune est particulièrement visible est la reconstruction de coraux par images sous-marines. Des images de l'organisme marin sont prises en stéréographie à l'aide d'un robot possédant deux caméras, et une reconstitution 3D est ensuite effectuée en comparant les images prises par les deux caméras pour déterminer la profondeur de chaque point dans les images.
Cette détermination de la profondeur passe par un \textit{matching} entre les deux images : pour certains points notables dans l'image de gauche, appelés points d'intérêt, le point correspondant sur l'image de droite doit être trouvé.
Pour avoir une grande précision dans cette reconstitution ($< 1$mm, ce qui n'est pas atteint à l'heure actuelle), il est nécessaire d'avoir un grand nombre de points ($N > 3000$ sur des images de résolution $3000 * 2000^$) dont la correspondance est exacte. 
Or SIFT donne un très grand nombre de correspondances ($N > 10 000$) sur ce type d'images, mais dont un très grand nombre sont erronées, ce qui fait baisser la précision de la reconstruction d'un facteur $10$, voire plus.
Nous avons donc souhaité améliorer la détection et la caractérisation de points d'intérêt dans des images sous-marines pour pouvoir augmenter la précision des méthodes de reconstitution d'organismes sous-marins, qui sont amenées à se développer aujourd'hui pour faire face à la perte massive de récifs coraliens.

\subsection{Objectif}

\begin{equation}
	\cos^3 \theta =\frac{1}{4}\cos\theta+\frac{3}{4}\cos 3\theta
	\label{eq:example}
\end{equation}

Automatically referencing an equation number using its label: Equation \ref{eq:example}.

In hac habitasse platea dictumst. Curabitur mattis elit sit amet justo luctus vestibulum. In hac habitasse platea dictumst. Pellentesque lobortis justo enim, a condimentum massa tempor eu. Ut quis nulla a quam pretium eleifend nec eu nisl. Nam cursus porttitor eros, sed luctus ligula convallis quis. Nam convallis, ligula in auctor euismod, ligula mauris fringilla tellus, et egestas mauris odio eget diam. Praesent sodales in ipsum eu dictum. Aenean vel enim ipsum. Fusce ut felis at eros sagittis bibendum mollis lobortis libero.

Maecenas consectetur metus at tellus finibus condimentum. Proin arcu lectus, ultrices non tincidunt et, tincidunt ut quam. Integer luctus posuere est, non maximus ante dignissim quis. Nunc a cursus erat. Curabitur suscipit nibh in tincidunt sagittis. Nam malesuada vestibulum quam id gravida. Proin ut dapibus velit. Vestibulum eget quam quis ipsum semper convallis. Duis consectetur nibh ac diam dignissim, id condimentum enim dictum. Nam aliquet ligula eu magna pellentesque, nec sagittis leo lobortis. Aenean tincidunt dignissim egestas.

%------------------------------------------------
\section{Etat de l'art}

Dans cette section, nous présentons les algorithmes de détection et caractérisation de points d'intérêt déjà développés dans la littérature.

\subsection{SIFT}

SIFT (\textit{Scale Invariant Feature Keypoints}) [\ref{article:sift}], algorithme fondateur dans le domaine, utilise les extrema de l'espace des échelles pour déterminer des points d'intérêt dans l'image.
L'espace des échelles de l'image est calculé en effectuant des différences de gausiennes, i.e. des différences pixel par pixel de l'image convoluée par des filtres gaussiens d'écart-type $\sigma$ variable.
Un pixel à une échelle $\sigma_i$ est alors considéré comme un point d'intérêt si et seulement si sa valeur est supérieure (resp. inférieure) aux 8 pixels voisins à l'échelle $\sigma_i$, aux 9 pixels voisins à l'échelle $\sigma_{i+1}$ et aux 9 pixels voisins à l'échelle $\sigma_{i-1}$ (c'est alors un extremum de l'espace des échelles).
Après une étape de filtrage intermédiaire, les points d'intérêt trouvés sont alors caractérisés par un histogramme d'orientation des gradients autour du point. 
Le détail de l'assignation d'une orientation et d'une magnitude aux gradients des voisins est donné par les équations \ref{eq:gradient_orientation}.
Chaque point d'intérêt est ainsi représenté par un descripteur de dimension 128, contenant les informations d'un carré de 4x4 histogrammes autour du point, avec 8 directions enregistrées dans chaque cellule. (INSERER IMAGE p.15 article SIFT)

\begin{figure}
	\centering
	\begin{subfigure}[H]{\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{images/sift_descriptor.png}
	\end{subfigure}
	\label{figure:sift_descriptor}
	\caption{Illustration du descripteur utilisé dans SIFT}
\end{figure}

\subsection{SURF}

\subsection{DAISY}

\section{Données}

Dans cette section, nous présentons les images que nous avons à traiter dans le cadre de la stéréographie sous-marine.

\subsection{Images réelles}

Les images réelles dont nous disposons proviennent d'une expérience en bassin peu profond, dans lequel un rocher a été photographié par un robot à deux caméras. Un exemple d'image est disponible en figure \ref{figure:irl_example}.
Les différences majeures entre des images en surface et des images sous-marines sont une perte de contraste significative et une modification des couleurs (visible à l'oeil nu).

\begin{figure}
	\centering
	\begin{subfigure}[H]{\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{images/irl_example.png}
	\end{subfigure}
	\label{figure:irl_example}
	\caption{Une image réelle expérimentale}
\end{figure}

Les paramètres des images et des caméras sont résumés dans le tableau  :

\begin{table*}[t]
	\begin{adjustbox}{width=\columnwidth, left}
		\begin{tabular}{l c}
			\hline
			Résolution 	 & ?? \\
			\hline
			Angle entre caméras         & $10$°        \\
			Distance focale         & ??                   \\
			Distance à l'objet  & 1m \\
			\hline
		\end{tabular}
	\end{adjustbox}
	\label{table:params_irl_imgs}
	\caption{Paramètres des images réelles expérimentales}
\end{table*}

\subsection{Images de synthèse}

Pour pouvoir comparer les performances des descripteurs de l'état de l'art et le descripteur que nous avons conçu, nous avons décidé de créer des images de synthèse à l'aide du logiciel open-source Blender [\ref{cite:blender}], pour créer une scène 3D dans laquelle tous les paramètres seraient connus.
Un rocher texturé a été simulé dans cette scène 3D à l'aide de déformations à bruit gaussien, et deux caméras ont été placées pour simuler la stéréographie sous-marine.
Les scènes sont disponibles sur le dépôt du projet, nous donnons ici les principaux paramètres des scènes en table \ref{table:params_syn_imgs}

\begin{table*}[t]
	\begin{adjustbox}{width=\columnwidth, left}
		\begin{tabular}{l c}
			\hline
			Résolution 	 & ?? \\
			\hline
			Angle entre caméras         & $10$°        \\
			Distance focale         & ??                   \\
			Distance à l'objet  & 1m \\
			\hline
		\end{tabular}
	\end{adjustbox}
	\label{table:params_syn_imgs}
	\caption{Paramètres des images de synthèse}
\end{table*}

\section{Méthodologie}

\subsection{Traitement de l'image}
Les images sont transformées en niveau de gris par simple moyenne des $3$ canaux de couleur \textit{RGB}.
Parce qu'on se focalise uniquement sur des point appartenant à la chose photographiée, on procède
à un agrandissement manuel dessus, afin de minimiser l'arrière-plan. Ensuite un filtre gaussien est
appliqué, d'écart-type $1$ et uniquement sur les images de synthèse, car les images réelles
dont nous disposons sont selon nous suffisamment précises et peu contrastées,
et car nous jugeons notre solution sensible au manque de contraste.

\subsection{Notion de point d'intérêt}

Il est certes possible de chercher les paires de points de façon brute entre
l'entièreté des pixels de chaque image, mais c'est insensé sur le
point de vue du temps de calcul, et clairement inutile: beaucoup de
points réels ne présentent que peu d'intérêt car la surface ie est
localement plane.

Il nous faut donc nous poser la question de ce qu'est un bon point
d'intérêt, ie un point dont le voisinage topologique varie de façon
assez significative pour être reconnaissable malgré un changement de
point d'observation, ie un changement de relief apparent et de contraste.

SIFT se concentre sur la répartition des gradients
dans le voisinage, et élimine des points sur des arêtes, jugés trop
sensibles au bruit, cette caractérisation des arêtes se fait par
le calcul de la trace et du déterminant de la hessienne,
qui sont fonctions des courbures principales.
Les courbures principales en un point sont les extrema locaux des courbures d'un voisinage de la surface
autour du point. Elles
coïncident avec les valeurs propres de la hessienne de la surface en le point,
et leurs directions respectives sont les vecteurs propres associés.

Nous pensons que ces courbures principales méritent plus d'attention,
et pourraient être considérées telles quelles. Dit grossièrement,
elles apportent une information à l'ordre $2$ sur la surface et complètent
l'information d'ordre $1$ fournie par le gradient. Un bon
point d'intérêt a selon nous dans son voisinage une variation marquée des courbures.


C'est pour cela que nous ne gardons que $30 \%$ des pixels de chaque image,
de plus grande courbure principale moyenne (en norme).

\subsection{Descripteur}
Inspiré de SIFT, nous avons cherché pour un pixel donné à en construire un descripteur,
ie à encoder dans un vecteur des caractéristiques topologiques de son voisinage, qu'on espère trouver
sur les deux images le plus à l'identique possible.

Définissons d'abord des propriétés de robustesse
que nous souhaitons pour notre descripteur:

\begin{enumerate}
	\item Invariance par échelle: pourtant un atout essentiel de SIFT, nous l'avons mise
	      de côté lors de notre développement. La raison est que cette propriété
	      répond à un besoin très général d'identifier des points d'intérêt à différentes
	      échelles. C'est très utile lorsque l'objet ciblé présente une structure et
	      un relief très variés, par exemple une maison. Mais dans notre utilisation précise,
	      nous voulons à terme photographier des récifs coraliens, et actuellement, en
	      phase expérimentale, nous utilisons des rochers. Le relief d'un rocher ne présente
	      pas de motif notable, comme des arêtes marquées, mais tend au contraire à être
	      uniforme et répétitif. Puisque nous voulons avoir un nombre conséquent de bons points
	      appariés, nous avons décidé de nous focaliser sur une taille fixe de points d'intérêt,
	      avec l'espoir de pouvoir calibrer cette taille de façon expérimentale en fonction
	      des conditions de nos observations.
	\item Invariance par rotation: le robot sous-marin n'est pas parfaitement
	      horizontal lorsqu'il photographie, une même forme n'aura pas la même
	      orientation sur les deux images. Nous souhaitons donc pouvoir identifier un
	      point, même si son voisinage subit une rotation dans le plan de photographie.
	\item Robustesse à la luminosité: la lumière sous-marine est naturellement inégale,
	      que ce soit à cause de la lumière du soleil qui est réfractée dans l'eau, ou bien à
	      cause des éclairages du robot sous-marin.
\end{enumerate}


Le descripteur repose principalement sur le gradient et les courbures principales.
Ces valeurs sont approximées par différences finies symétriques de l'image à l'ordre $1$ et $2$.
Dans un repère classique de l'image numérique $I$ de dimension $(w, h) \in \mathbb{N}^2$,
vue comme une fonction discrète:
$I: \llbracket 0, w-1 \rrbracket \times \llbracket 0, h-1 \rrbracket \rightarrow [0, 1]$
d'abscisses $x$ et d'ordonnées $y$, la norme du gradient
et son orientation sont approximées par:

\begin{multline}
	m(x, y) = \\
	\sqrt{(I(x+1, y) - I(x-1,y))^2 + (I(x,y+1)-I(x,y-1))^2}
\end{multline}

\begin{multline}
	\theta(x, y) = \\
	\tan^{-1}\left(\frac{I(x,y+1)-I(x,y-1)}{I(x+1, y) - I(x-1,y)}\right)
\end{multline}

Pour calculer les courbures et directions principales, la hessienne $H$ est d'abord calculée
similairement, voir l'Equation~\ref{eq:hess_coefs} en Annexes :

\begin{equation}
	H = \\
	\begin{bmatrix}
		dxx & dxy \\
		dxy & dyy
	\end{bmatrix}
\end{equation}

Puis le module \textit{Numpy} de \textit{Python} pour en obtenir les valeurs et vecteurs
propres.



Construisons le voisinage autour d'un pixel. En appelant $\theta$
l'orientation du gradient en le point, le voisinage en question est l'image par
une rotation d'angle $\theta$ d'un carré de $15\cdot 15$ pixels,
centré en le pixel. Cela permet d'être invariant par rotation des images.
Ce voisinage est ensuite découpé en $9$ cellules de $25$ pixels.

Pour construire le descripteur, en chacune des $9$ cellules, on va
calculer l'histogramme des gradients des pixels,
répartis selon l'écart entre leur orientation et celle du pixel central.
La résolution angulaire de l'orientation est $5$°. La contribution d'un pixel
est son gradient normalisé par celui du pixel central, pondéré par une gaussienne
de sa distance au centre, d'écart type la moitié du voisinage, ici $7.5$. Les contributions sont ensuite transformées en pourcentage
de la somme des contributions dans toutes les cellules.

On calcule de la même façon les histogrammes des courbures principales.

En concaténant les $3$ histogrammes, on obtient le descripteur du pixel,
vecteur de taille $360 / 5 \cdot 3 \cdot 5 \cdot 1944$.


\subsection{Appariement}
Bien qu'il existe des méthode analogues aux plus proches voisins adaptées à de la haute dimension,
comme la \textit{Best Bin First}, nous ne les avons pas utilisées car nous ne les avons pas
recréées nous-mêmes et celles des librairies Python ne pouvaient pas être précompilées en C
par Numba.

Nous avons donc dû faire avec l'appariement brut: pour chaque pixel filtré de l'image de gauche,
on l'associe à celui de l'image de droite ayant le descripteur le plus similaire,
dans le sens où il minimise leur distance euclidienne.
Ce choix de distance a été fait par défaut et nous n'avons pas testé d'autres distances.

Il pourrait être intéressant d'expérimenter avec une norme pondérée, accordant plus d'importance
à l'information des gradients plutôt qu'à celle des courbures.

\subsection{Postfiltrage}
Une fois les paires calculées, nous n'en gardons que $2 \%$ du nombre de pixels de l'image,
ayant les plus petites distances, ie les paires avec le plus haut taux
de confiance. Nous avons choisi ce seuil puisqu'il correspond à un peu plus que
notre objectif de $3000$ bonnes paires sur nos images réelles, de résolution
$900 \cdot 1400$.
Nous l'avons aussi
utilisé sur nos images de synthèses, dont la résolution est pourtant
inférieure.

Par commodité, on désignera notre algorithme
par \textit{HM}, pour \textit{Home Made}.

%------------------------------------------------

\begin{table*}[t]
	\centering
	\begin{adjustbox}{width=\textwidth,center}
		\begin{tabular}{l c c c c}
			\hline
			Angle entre caméras                                              & \multicolumn{2}{c}{$10$°} & \multicolumn{2}{c}{$7$°}                                              \\
			Algorithme (préfiltrage)                                         & \textit{HM} ($60\%$)      & SIFT                     & \textit{HM} ($70\%$) & SIFT                \\
			\hline\makecell[l]{Nombre de bonnes paires parmi les                                                                                                                 \\
			$3480$ de distance min. oui}                                     & $2579$ ($74.11 \%$)       & $613$ ($17.61 \%$)       & $2532$ ($72.76 \%$)  & $604$ ($17.36 \%$)  \\
			\hline
			\makecell[l]{Statistiques sur les distances                                                                                                                          \\
			des bonnes (B) et mauvaises (M) paires}                          & (B)   (M)                 & (B)   (M)                & (B)   (M)            & (B)   (M)           \\
			\makecell[l]{Min}                                                & $10.98$   $14.11$         & $27.48$   $33.18$        & $11.97$   $12.35$    & $15.56$   $38.74$   \\
			\makecell[l]{Max}                                                & $33.69$   $44.50$         & $255.22$   $264.50$      & $32.45$   $43.46$    & $253.92$   $251.11$ \\
			\makecell[l]{Moyenne}                                            & $20.82$   $23.24$         & $111.58$   $124.36$      & $20.79$   $23.28$    & $112.42$   $121.08$ \\
			\makecell[l]{Ecart-type}                                         & $1.99$   $2.72$           & $49.09$   $52.81$        & $2.03$   $2.75$      & $50.70$   $48.92$   \\
			\hline
			\makecell[l]{Résultats avant postfiltrage (Sélection des $2\%$)} &                           &                          &                      &                     \\
			\makecell[l]{Total de paires calculées}                          & 69600                     & 931                      & 52200                & 930                 \\
			\makecell[l]{Total de bonnes paires renvoyées (\% de paires)}    & 17110 ($24.58\%$)         & 613 ($65.84 \%$)         & 12647($24.23\%$)     & 604   ($64.94\%$)   \\
			\hline
			\makecell[l]{Temps de calcul (hh:mm:ss) :                                                                                                                            \\
			Image $1$, Image $2$, Appariement}                               & \makecell{00:15:39,
			\\ 00:15:27, \\ 00:12:05}     &  Total $<5$s                       &    \makecell{00:11:48,\\ 00:11:37, \\ 00:09:24}                 &       Total $<5$s                \\
			\hline
		\end{tabular}
	\end{adjustbox}
	\caption{Comparaison des résultats sur deux paires d'images de synthèse de résolution $900 \cdot 1400$
		de notre algorithme \textit{HM} par rapport à SIFT réglé par défaut.
		Les bonnes paires ont été identifiée par notre algorithme Blender.}
	\label{table:res_syn}
\end{table*}


\section{Résultats}

Nous avons comparé \textit{HM} avec SIFT, paramétré par défaut, ie avec un seuil de contraste de $0.04$,
un seuil d'arête de $10$, un écart type de gaussienne de $1.6$, et un seuil de distance de $0.75$.
Il aurait été intéressant d'essayer d'optimiser SIFT sur nos images de synthèse.

\subsection{Résultats sur image de synthèse}

Pour des sous images de synthèses de résolution $300 \cdot 580$
d'angles entre caméras de $10$° et $7$° degrés, les résultats sont dans le Tableau~\ref{table:res_syn}.
Suprenamment, \textit{HM} a d'excellents résultats, puisque la proportion de bonnes paires parmi celles gardées
est haute par rapport à celle de SIFT.
Il est rassurant d'avoir une distance moyenne des bonnes paires en dessous de celle des mauvaises, de plus
l'écart entre les deux moyennes est d'environ $1.22$ fois l'écart-type des bonnes paires (pour $10$°). Pour SIFT en comparaison,
il est seulement de $0.27$ fois.
On en conclut que sur ce test, notre descripteur a été plus discriminant que SIFT.

Quand on regarde les résultats pour $10$° avant d'avoir fait le postfiltrage, donc avant de ne garder que l'équivalent des $2\%$ des pixels de l'image,
on voit que \textit{HM} renvoie comme attendu un grand nombre de paires, et la bonne nouvelle est que parmi elles se trouvent relativement beaucoup
de bonnes paires: $24.58\%$. A contrario, SIFT a la vertu d'être très parcimonieux et précis avec $931$ paires dont $65.8\%$ de bonnes.
Cependant, le reproche qu'on peut faire à SIFT est de ne pas renvoyer assez de bonnes paires, bien que le pourcentage de bonnes paires
parmi le nombre de pixels de la sous-image est de $0.31\%$, supérieur à notre objectif de $0.24\%$ correspondant à 3000 points dans une
image $900 \cdot 1400$. Ainsi, SIFT valide dans notre cas nos attentes, mais de peu, là où \textit{HM} peut potentiellemnt détecter bien
plus de bons points.

On peut voir sur la Figure~\ref{figure:fig_syn} des paires proposées par \textit{HM}.
Un effet du préfiltrage par la courbure principale moyenne est que \textit{HM} ne renvoie pas
de paires dans les zones peu contrastées, là où SIFT en trouve plus, voir Figure~\ref{figure:fig_syn_sift}
en Annexes.

\begin{figure}
	\centering
	\begin{subfigure}[H]{\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{images/res1_g.png}
	\end{subfigure}

	\begin{subfigure}[H]{\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{images/res1_d.png}
	\end{subfigure}
	\caption{$250$ paires tirées aléatoirement parmi celles proposées par \textit{HM}.}
	\label{figure:fig_syn}
\end{figure}

\begin{figure*}
	\centering
	\begin{subfigure}[H]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/irl_z2_g.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}[H]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/irl_z2_d.png}
	\end{subfigure}
	\caption{Zoom sur des paires tirées aléatoirement parmi celles proposées par \textit{HM} sur images réelles.}
	\label{figure:fig_irl}
\end{figure*}

\subsection{Résultats sur images réelles}
Les images réelles dont nous disposons ont comme résolution $800 \cdot 1500$ après recadrage. \textit{HM} a fonctionné
avec un préfiltrage de $70 \%$, le temps de calcul a été de 4 heures 30 par image et 5 heures 10 pour l'appariement.
On voit clairement ici la limite de notre algorithme, peu parcimonieux par nature et implémenté en Python.
Quant aux résultats, les zooms en Figure~\ref{figure:fig_irl} montrent de nombreux déchets, malgré de visiblement
bonnes paires dans les zones contrastées.
On voit tout d'abord que nos images de synthèses sont sensiblement différentes des réelles,
en terme d'angle entre caméras, de variation de contraste, et de résolution.
On comprend ici la faiblesse de \textit{HM} sur ces dernières qui semble observer un voisinage fixe,
trop petit ici pour encapsuler un relief topologique caractéristique du point,
car ces images sont de haute résolution et possèdent globalement peu de relief, donc les
variations notables de celui-ci se font sur une plus grande zone que $15 \cdot 15$.
On trouvera en Annexes sur la Figure~\ref{figure:fig_irl_sift} les résultats de SIFT sur la même zone.
Sans surprise, ceux-ci sont très bons, avec très peu de déchets flagrants, il faudrait certes zoomer plus
pour vérifier la validité des paires au pixel près, mais il reste que nos performances avec \textit{HM}
restent bien en deçà de celles de SIFT. S'il faut néanmoins trouver un avantage à \textit{HM}, on fera
remarquer que celui-ci a trouvé quelques visiblement bonnes paires, non identifiées par SIFT, par
exemple la paire bleue en bas à gauche de la Figure~\ref{figure:fig_irl}.
Mais évidemment, notre algorithme \textit{HM} n'a pas les résultats attendus.


\clearpage

\section{Discussion}

test

This statement requires citation \autocite{Smith:2023qr}. This statement requires multiple citations \autocite{Smith:2023qr, Smith:2024jd}. This statement contains an in-text citation, for directly referring to a citation like so: \textcite{Smith:2024jd}.



\section{Annexes}

\begin{equation} \label{eq:hess_coefs}
	\begin{split}
		dxx & = I(x+1, y) + I(y, x-1) - 2I(x, y),                                              \\
		dyy & = I(x, y+1) + I(y-1, x) - 2I(x, y),                                              \\
		dxy & = \frac{\left(I(x+1; y+1) + I(x-1, y-1) - I(x-1, y+1) - I(x+1, y-1) \right)}{4}.
	\end{split}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/histo_result.png}
	\caption{Exemple d'un des trois carrés d'histogrammes constituant le descripteur
		d'un point.}
	\label{figure:fig_hist}
\end{figure}

\clearpage

\begin{figure}[H]
	\centering
	\begin{subfigure}[H]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/res1_sift_g.png}
	\end{subfigure}
	\begin{subfigure}[H]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/res1_sift_d.png}
	\end{subfigure}
	\caption{$250$ paires tirées aléatoirement parmi celles proposées par SIFT.}
	\label{figure:fig_syn_sift}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{subfigure}[H]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/irl_sift_z1_g.png}
	\end{subfigure}
	\begin{subfigure}[H]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/irl_sift_z1_d.png}
	\end{subfigure}
	\caption{Zoom sur des paires tirées aléatoirement parmi celles proposées par SIFT.}
	\label{figure:fig_irl_sift}
\end{figure}

\clearpage

%----------------------------------------------------------------------------------------
%	 REFERENCES
%----------------------------------------------------------------------------------------

\printbibliography % Output the bibliography

%----------------------------------------------------------------------------------------

\end{document}
